{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3abf34",
   "metadata": {},
   "source": [
    "# Churn Prediction with XGBoost, LightGBM & CatBoost (Imbalanced Data)\n",
    "\n",
    "## Objective\n",
    "Replicate the full workflow of comparing XGBoost, LightGBM and CatBoost on an **imbalanced churn** dataset, including:\n",
    "- Comprehensive EDA\n",
    "- Preprocessing with two parallel approaches\n",
    "- Imbalance handling (SMOTE + class weights)\n",
    "- Model training/tuning \n",
    "- Threshold tuning and comprehensive evaluation\n",
    "- Feature importance and SHAP analysis\n",
    "\n",
    "**Primary metric:** ROC-AUC  \n",
    "**Secondary metrics:** Recall, Precision, F1, Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c275c",
   "metadata": {},
   "source": [
    "## 1. Imports + Versions + Seed Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (roc_auc_score, roc_curve, precision_recall_curve,\n",
    "                             classification_report, confusion_matrix, accuracy_score,\n",
    "                             precision_score, recall_score, f1_score)\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Boosting models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "\n",
    "# Print versions for version control\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn: {sns.__version__}\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5106cd1",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd564037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = Path('../data/churn.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\n=== FIRST 5 ROWS ===\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d6379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about the dataset\n",
    "print(\"=== DATASET INFO ===\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n=== DESCRIPTIVE STATISTICS ===\")\n",
    "display(df.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfe02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable format and standardize\n",
    "print(\"=== TARGET VARIABLE ANALYSIS ===\")\n",
    "print(f\"Target column 'Churn' unique values: {df['Churn'].unique()}\")\n",
    "print(f\"Target value counts:\\n{df['Churn'].value_counts()}\")\n",
    "\n",
    "# Standardize target to {0,1} if needed\n",
    "if df['Churn'].dtype == 'object':\n",
    "    print(\"\\nConverting text target to binary...\")\n",
    "    df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})\n",
    "    print(f\"After conversion: {df['Churn'].unique()}\")\n",
    "\n",
    "# set target is integer\n",
    "df['Churn'] = df['Churn'].astype(int)\n",
    "\n",
    "# Check for missing values in target\n",
    "print(f\"\\nMissing values in target: {df['Churn'].isna().sum()}\")\n",
    "print(f\"Final target distribution:\\n{df['Churn'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94357a",
   "metadata": {},
   "source": [
    "## 3. Initial EDA - Target Distribution & Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot\n",
    "target_counts = df['Churn'].value_counts()\n",
    "ax1.bar(['No Churn (0)', 'Churn (1)'], target_counts.values, \n",
    "        color=['skyblue', 'salmon'], alpha=0.8)\n",
    "ax1.set_title('Target Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Count')\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    ax1.text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(target_counts.values, labels=['No Churn (0)', 'Churn (1)'], \n",
    "        autopct='%1.1f%%', colors=['skyblue', 'salmon'], startangle=90)\n",
    "ax2.set_title('Target Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate imbalance metrics\n",
    "n_majority = target_counts[0]\n",
    "n_minority = target_counts[1]\n",
    "imbalance_ratio = n_majority / n_minority\n",
    "\n",
    "print(f\"\\n=== IMBALANCE ANALYSIS ===\")\n",
    "print(f\"Majority class (No Churn): {n_majority:,} samples ({n_majority/len(df)*100:.1f}%)\")\n",
    "print(f\"Minority class (Churn): {n_minority:,} samples ({n_minority/len(df)*100:.1f}%)\")\n",
    "print(f\"Imbalance ratio (majority/minority): {imbalance_ratio:.2f}:1\")\n",
    "print(f\"This is a {'highly ' if imbalance_ratio > 5 else ''}imbalanced dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b6362e",
   "metadata": {},
   "source": [
    "## 4. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad4b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "missing_values = df.isna().sum().sort_values(ascending=False)\n",
    "missing_percent = (missing_values / len(df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Missing_Percentage': missing_percent\n",
    "})\n",
    "\n",
    "print(\"=== MISSING VALUES ANALYSIS ===\")\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "if missing_df['Missing_Count'].sum() > 0:\n",
    "    # Visualize missing values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_cols = missing_df[missing_df['Missing_Count'] > 0]\n",
    "    \n",
    "    plt.bar(range(len(missing_cols)), missing_cols['Missing_Percentage'])\n",
    "    plt.xticks(range(len(missing_cols)), missing_cols.index, rotation=45)\n",
    "    plt.title('Missing Values by Column', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Missing Percentage (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d71b7f",
   "metadata": {},
   "source": [
    "## 5. Feature Types Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c28793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature types\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target from numeric if present\n",
    "if 'Churn' in numeric_features:\n",
    "    numeric_features.remove('Churn')\n",
    "\n",
    "# Remove ID columns from categorical if present\n",
    "id_columns = [col for col in categorical_features if 'id' in col.lower() or 'ID' in col]\n",
    "categorical_features = [col for col in categorical_features if col not in id_columns]\n",
    "\n",
    "print(\"=== FEATURE TYPES ANALYSIS ===\")\n",
    "print(f\"Total features: {len(df.columns) - 1}\")\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"ID columns to drop ({len(id_columns)}): {id_columns}\")\n",
    "\n",
    "# Store that we will use later\n",
    "feature_types = {\n",
    "    'numeric': numeric_features,\n",
    "    'categorical': categorical_features,\n",
    "    'id_columns': id_columns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263396c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop customerID column \n",
    "df = df.drop('customerID', axis=1)\n",
    "print(f\"Shape after dropping customerID: {df.shape}\")\n",
    "print(f\"Columns remaining: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ecd484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update feature type lists after dropping customerID\n",
    "categorical_features = [col for col in categorical_features if col in df.columns]\n",
    "id_columns = []  # Clear since we dropped customerID\n",
    "\n",
    "print(\"=== UPDATED FEATURE TYPES ===\")\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"ID columns ({len(id_columns)}): {id_columns}\")\n",
    "print(f\"\\\\nTotal features for modeling: {len(numeric_features) + len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c126b03d",
   "metadata": {},
   "source": [
    "## 6. Numeric Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric features correlation heatmap\n",
    "if len(numeric_features) > 1:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    correlation_matrix = df[numeric_features].corr()\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5)\n",
    "    plt.title('Correlation Matrix - Numeric Features', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Let's see highest correlations:\n",
    "    corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "    \n",
    "    corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    print(\"\\n=== TOP CORRELATIONS ===\")\n",
    "    for feat1, feat2, corr in corr_pairs[:5]:\n",
    "        print(f\"{feat1} <-> {feat2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate analysis for key numeric features\n",
    "key_numeric = [col for col in ['tenure', 'MonthlyCharges', 'TotalCharges'] if col in numeric_features]\n",
    "\n",
    "if len(key_numeric) > 0:\n",
    "    fig, axes = plt.subplots(2, len(key_numeric), figsize=(5*len(key_numeric), 10))\n",
    "    if len(key_numeric) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, feature in enumerate(key_numeric):\n",
    "        # Histogram\n",
    "        axes[0, i].hist(df[feature].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[0, i].set_title(f'{feature} - Distribution', fontweight='bold')\n",
    "        axes[0, i].set_xlabel(feature)\n",
    "        axes[0, i].set_ylabel('Frequency')\n",
    "        \n",
    "        # Box plot by target\n",
    "        df_clean = df[[feature, 'Churn']].dropna()\n",
    "        sns.boxplot(data=df_clean, x='Churn', y=feature, ax=axes[1, i])\n",
    "        axes[1, i].set_title(f'{feature} - by Churn Status', fontweight='bold')\n",
    "        axes[1, i].set_xlabel('Churn (0=No, 1=Yes)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics by target\n",
    "    print(\"\\n=== NUMERIC FEATURES BY TARGET ===\")\n",
    "    for feature in key_numeric:\n",
    "        print(f\"\\n{feature}:\")\n",
    "        summary = df.groupby('Churn')[feature].agg(['mean', 'median', 'std']).round(2)\n",
    "        print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539cd2af",
   "metadata": {},
   "source": [
    "## 7. Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "print(\" CATEGORICAL FEATURES SUMMARY \")\n",
    "for feature in categorical_features[:10]:  # Show first 10 \n",
    "    unique_count = df[feature].nunique()\n",
    "    print(f\"{feature}: {unique_count} unique values\")\n",
    "    if unique_count <= 10:\n",
    "        print(f\"  Values: {list(df[feature].unique())}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key categorical features\n",
    "key_categorical = [col for col in ['Contract', 'InternetService', 'PaymentMethod', 'gender', 'SeniorCitizen'] \n",
    "                   if col in categorical_features or col in numeric_features]\n",
    "\n",
    "if len(key_categorical) > 0:\n",
    "    n_cols = 2\n",
    "    n_rows = (len(key_categorical) + 1) // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    for i, feature in enumerate(key_categorical):\n",
    "        if i < len(axes):\n",
    "            # Count plot with churn rate\n",
    "            df_temp = df.groupby(feature)['Churn'].agg(['count', 'mean']).reset_index()\n",
    "            \n",
    "            ax = axes[i]\n",
    "            bars = ax.bar(df_temp[feature], df_temp['count'], alpha=0.7)\n",
    "            \n",
    "            # Add churn rate as text on bars\n",
    "            for j, (bar, churn_rate) in enumerate(zip(bars, df_temp['mean'])):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                       f'{churn_rate:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            ax.set_title(f'{feature} - Count & Churn Rate', fontweight='bold')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(key_categorical), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a7cb33",
   "metadata": {},
   "source": [
    "## 8. Data Quality Assessment & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data quality summary\n",
    "print(\" DATA QUALITY SUMMARY \")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Missing values: {df.isna().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "print(f\"Target distribution: {df['Churn'].value_counts().to_dict()}\")\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "print(f\"\\n✅ Data preprocessing completed - Ready for model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01f16e",
   "metadata": {},
   "source": [
    "# Phase 2: Data Preprocessing & Feature Engineering\n",
    "\n",
    "## 9. Data Cleaning & Type Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e636fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and fix TotalCharges column (space sebepli sorun yaptı bak buna!!!)\n",
    "print(\"=== DATA CLEANING & TYPE FIXES ===\")\n",
    "print(f\"TotalCharges data type: {df['TotalCharges'].dtype}\")\n",
    "print(f\"TotalCharges unique values (first 10): {df['TotalCharges'].unique()[:10]}\")\n",
    "\n",
    "# Check for non-numeric values in TotalCharges\n",
    "non_numeric_mask = pd.to_numeric(df['TotalCharges'], errors='coerce').isna()\n",
    "non_numeric_count = non_numeric_mask.sum()\n",
    "print(f\"\\\\nNon-numeric values in TotalCharges: {non_numeric_count}\")\n",
    "\n",
    "if non_numeric_count > 0:\n",
    "    print(f\"Non-numeric values: {df.loc[non_numeric_mask, 'TotalCharges'].unique()}\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(f\"\\\\nExamples of rows with non-numeric TotalCharges:\")\n",
    "    print(df.loc[non_numeric_mask, ['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn']].head())\n",
    "    \n",
    "    # Convert spaces to NaN, then handle missing values\n",
    "    df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan)\n",
    "    \n",
    "    # Check missing values after conversion\n",
    "    missing_total_charges = df['TotalCharges'].isna().sum()\n",
    "    print(f\"\\\\nMissing values in TotalCharges after cleaning: {missing_total_charges}\")\n",
    "    \n",
    "    if missing_total_charges > 0:\n",
    "        # For customers with 0 tenure, TotalCharges should be 0\n",
    "        # For others, we can impute or drop\n",
    "        zero_tenure_mask = (df['tenure'] == 0) & df['TotalCharges'].isna()\n",
    "        zero_tenure_count = zero_tenure_mask.sum()\n",
    "        print(f\"Customers with 0 tenure and missing TotalCharges: {zero_tenure_count}\")\n",
    "        \n",
    "        # Fill missing TotalCharges with 0 for zero tenure customers\n",
    "        df.loc[zero_tenure_mask, 'TotalCharges'] = '0'\n",
    "        \n",
    "        # For remaining missing values, drop rows (since they're few)\n",
    "        remaining_missing = df['TotalCharges'].isna().sum()\n",
    "        if remaining_missing > 0:\n",
    "            print(f\"Dropping {remaining_missing} rows with missing TotalCharges\")\n",
    "            df = df.dropna(subset=['TotalCharges'])\n",
    "\n",
    "# Convert TotalCharges to numeric\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'])\n",
    "\n",
    "print(f\"\\\\nFinal TotalCharges data type: {df['TotalCharges'].dtype}\")\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"TotalCharges stats: min={df['TotalCharges'].min():.2f}, max={df['TotalCharges'].max():.2f}, mean={df['TotalCharges'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6cc5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update feature types after cleaning\n",
    "print(\" UPDATED FEATURE TYPES AFTER CLEANING \")\n",
    "\n",
    "# Update numeric and categorical features\n",
    "numeric_features = ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "categorical_features = [col for col in categorical_features if col != 'TotalCharges']\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"Total features: {len(numeric_features) + len(categorical_features)}\")\n",
    "\n",
    "# Verify data types\n",
    "print(f\"\\\\n Data types verification:\")\n",
    "for col in numeric_features:\n",
    "    print(f\"  {col}: {df[col].dtype}\")\n",
    "\n",
    "# Check for any remaining data quality issues\n",
    "print(f\"\\\\n Final data quality check:\")\n",
    "print(f\"Missing values: {df.isna().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "print(f\"Target distribution: {df['Churn'].value_counts().to_dict()}\")\n",
    "\n",
    "# Update the feature_types dictionary\n",
    "feature_types = {\n",
    "    'numeric': numeric_features,\n",
    "    'categorical': categorical_features,\n",
    "    'id_columns': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd6654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle duplicate rows\n",
    "print(\" HANDLING DUPLICATE ROWS\")\n",
    "print(f\"Duplicate rows found: {df.duplicated().sum()}\")\n",
    "\n",
    "if df.duplicated().sum() > 0:\n",
    "    # Show some examples of duplicates\n",
    "    duplicate_rows = df[df.duplicated(keep=False)].sort_values(by=df.columns.tolist())\n",
    "    print(f\"\\\\n Example duplicate rows (first 5):\")\n",
    "    print(duplicate_rows.head())\n",
    "    \n",
    "    # Remove duplicates, keeping the first occurrence\n",
    "    df_before = df.shape[0]\n",
    "    df = df.drop_duplicates()\n",
    "    df_after = df.shape[0]\n",
    "    \n",
    "    print(f\"\\\\nRemoved {df_before - df_after} duplicate rows\")\n",
    "    print(f\"Dataset shape after deduplication: {df.shape}\")\n",
    "    \n",
    "    # Check target distribution after removing duplicates\n",
    "    print(f\"Target distribution after deduplication: {df['Churn'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Update imbalance ratio\n",
    "    n_majority = (df['Churn'] == 0).sum()\n",
    "    n_minority = (df['Churn'] == 1).sum()\n",
    "    imbalance_ratio = n_majority / n_minority\n",
    "    print(f\"Updated imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89884974",
   "metadata": {},
   "source": [
    "## 10. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74692a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train/test split\n",
    "print(\" TRAIN/TEST SPLIT \")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "# Stratified split to preserve class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({y_train.value_counts()[0]:,} no churn, {y_train.value_counts()[1]:,} churn)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({y_test.value_counts()[0]:,} no churn, {y_test.value_counts()[1]:,} churn)\")\n",
    "\n",
    "# Calculate final imbalance ratio on training set\n",
    "train_imbalance_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Training set imbalance ratio: {train_imbalance_ratio:.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b813aa",
   "metadata": {},
   "source": [
    "## 11. Preprocessing Strategy\n",
    "\n",
    "We'll implement two parallel preprocessing approaches to show how boosting models are typically compared:\n",
    "\n",
    "1. **CatBoost Path**: Native categorical handling - keep categoricals as object/category\n",
    "2. **XGB/LGBM Path**: One-Hot encoding - transform categoricals to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical indices for CatBoost (native categorical handling)\n",
    "print(\" CATBOOST PREPROCESSING SETUP \")\n",
    "\n",
    "# Create categorical indices based on X_train column order\n",
    "cat_indices = []\n",
    "feature_names = list(X_train.columns)\n",
    "\n",
    "for i, col in enumerate(feature_names):\n",
    "    if col in categorical_features:\n",
    "        cat_indices.append(i)\n",
    "\n",
    "print(f\"Feature order in X_train: {feature_names}\")\n",
    "print(f\"Categorical feature indices for CatBoost: {cat_indices}\")\n",
    "print(f\"Categorical features: {[feature_names[i] for i in cat_indices]}\")\n",
    "\n",
    "# Verify the mapping\n",
    "print(\"\\\\nCategorical feature mapping:\")\n",
    "for idx in cat_indices:\n",
    "    print(f\"  Index {idx}: {feature_names[idx]}\")\n",
    "\n",
    "print(f\"\\\\nNumeric features for CatBoost: {[col for col in feature_names if col not in categorical_features]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ColumnTransformer for XGBoost/LightGBM (One-Hot encoding)\n",
    "print(\"=== XGB/LGBM PREPROCESSING SETUP ===\")\n",
    "\n",
    "# Define preprocessor with One-Hot encoding for categoricals and passthrough for numerics\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(f\"Numeric features for passthrough: {numeric_features}\")\n",
    "print(f\"Categorical features for OneHot encoding: {categorical_features}\")\n",
    "\n",
    "# Fit the preprocessor on training data to see the output\n",
    "print(\"\\\\nFitting preprocessor on training data...\")\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "print(f\"Original X_train shape: {X_train.shape}\")\n",
    "print(f\"Preprocessed X_train shape: {X_train_preprocessed.shape}\")\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "try:\n",
    "    feature_names_preprocessed = preprocessor.get_feature_names_out()\n",
    "except AttributeError:\n",
    "    # if any problem with sklearn try this:\n",
    "    numeric_names = numeric_features\n",
    "    cat_encoder = preprocessor.named_transformers_['cat']\n",
    "    cat_names = cat_encoder.get_feature_names(categorical_features)\n",
    "    feature_names_preprocessed = numeric_names + list(cat_names)\n",
    "\n",
    "print(f\"Number of features after preprocessing: {len(feature_names_preprocessed)}\")\n",
    "print(f\"First 10 feature names: {feature_names_preprocessed[:10]}\")\n",
    "print(f\"Last 10 feature names: {feature_names_preprocessed[-10:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ed840",
   "metadata": {},
   "source": [
    "## 12. Class Imbalance Handling Strategy\n",
    "\n",
    "We'll implement two branches to compare different imbalance handling approaches:\n",
    "- **Branch A (Baseline)**: Class weights only\n",
    "- **Branch B (SMOTE)**: Synthetic Minority Oversampling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class imbalance metrics and setup class weights\n",
    "print(\" CLASS IMBALANCE ANALYSIS\")\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = y_train.value_counts().sort_index()\n",
    "n_samples = len(y_train)\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "print(f\"Training set class distribution:\")\n",
    "for class_label, count in class_counts.items():\n",
    "    percentage = count / n_samples * 100\n",
    "    print(f\"  Class {class_label}: {count:,} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio = class_counts[0] / class_counts[1]\n",
    "print(f\"\\\\nImbalance ratio (majority/minority): {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "# Calculate class weights for different methods\n",
    "# Method 1: Balanced class weights (sklearn style)\n",
    "class_weights_balanced = n_samples / (n_classes * class_counts)\n",
    "print(f\"\\\\nBalanced class weights:\")\n",
    "for class_label, weight in class_weights_balanced.items():\n",
    "    print(f\"  Class {class_label}: {weight:.3f}\")\n",
    "\n",
    "# Method 2: Scale_pos_weight for XGBoost (bunu kaggle'da gördüm dene bunu !!!)\n",
    "scale_pos_weight = class_counts[0] / class_counts[1]\n",
    "print(f\"\\\\nXGBoost scale_pos_weight: {scale_pos_weight:.3f}\")\n",
    "\n",
    "# Method 3: CatBoost class weights (proportional to inverse frequency)\n",
    "total_neg = class_counts[0]\n",
    "total_pos = class_counts[1]\n",
    "catboost_class_weights = [1.0, total_neg / total_pos]\n",
    "print(f\"CatBoost class weights [neg, pos]: {catboost_class_weights}\")\n",
    "\n",
    "# Store class weight configurations\n",
    "class_weight_configs = {\n",
    "    'balanced_weights': class_weights_balanced,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'catboost_weights': catboost_class_weights\n",
    "}\n",
    "\n",
    "print(f\"\\\\n Class weight configurations ready for all models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7225e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SMOTE pipeline for imbalanced learning (bu iş için ne kadar gerekli hala emin değilim justify etmek lazım)\n",
    "print(\" SMOTE PIPELINE SETUP \")\n",
    "\n",
    "# SMOTE configuration\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Test SMOTE on preprocessed data to verify it works\n",
    "print(\"Testing SMOTE on preprocessed training data...\")\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "print(f\"Original training set shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"SMOTE training set shape: {X_train_smote.shape}\")\n",
    "\n",
    "print(f\"\\\\nOriginal class distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"SMOTE class distribution: {pd.Series(y_train_smote).value_counts().to_dict()}\")\n",
    "\n",
    "# Calculate new proportions after SMOTE\n",
    "smote_counts = pd.Series(y_train_smote).value_counts()\n",
    "print(f\"\\\\nSMOTE proportions:\")\n",
    "for class_label, count in smote_counts.items():\n",
    "    percentage = count / len(y_train_smote) * 100\n",
    "    print(f\"  Class {class_label}: {count:,} samples ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\\\n SMOTE pipeline tested successfully\")\n",
    "print(f\"Strategy: Preprocessing → SMOTE → Model for XGB/LGBM\")\n",
    "print(f\"Note: For CatBoost, we'll use class weights instead of SMOTE to maintain categorical features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5156d52",
   "metadata": {},
   "source": [
    "# Phase 3: Imbalance Handling & Model Setup\n",
    "\n",
    "## 13. Hyperparameter Grids \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb03ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate grid size\n",
    "def calculate_grid_size(param_grid):\n",
    "    total = 1\n",
    "    for values in param_grid.values():\n",
    "        total *= len(values)\n",
    "    return total\n",
    "\n",
    "# XGBoost Hyperparameter Grid (Fast Version)\n",
    "print(\" XGBOOST HYPERPARAMETER GRID \")\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\":       [200, 400],\n",
    "    \"learning_rate\":      [0.05, 0.1],\n",
    "    \"max_depth\":          [3, 6],\n",
    "    \"scale_pos_weight\":   [1, scale_pos_weight]\n",
    "}\n",
    "\n",
    "grid_size = calculate_grid_size(xgb_param_grid)\n",
    "print(f\"XGBoost grid: {grid_size} combinations ({grid_size * 5} CV fits)\")\n",
    "print(f\"Scale_pos_weight values: [1, {scale_pos_weight:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Hyperparameter Grid (Fast Version)\n",
    "print(\"=== LIGHTGBM HYPERPARAMETER GRID ===\")\n",
    "\n",
    "lgbm_param_grid = {\n",
    "    \"n_estimators\":        [200, 400],\n",
    "    \"learning_rate\":       [0.05, 0.1],\n",
    "    \"max_depth\":           [-1, 6],\n",
    "    \"class_weight\":        [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "grid_size = calculate_grid_size(lgbm_param_grid)\n",
    "print(f\"LightGBM grid: {grid_size} combinations ({grid_size * 5} CV fits)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Hyperparameter Grid (Fast Version)\n",
    "print(\" CATBOOST HYPERPARAMETER GRID \")\n",
    "\n",
    "cat_param_grid = {\n",
    "    \"iterations\":        [500, 1000],\n",
    "    \"learning_rate\":     [0.05, 0.1],\n",
    "    \"depth\":             [4, 6],\n",
    "    \"class_weights\":     [None, catboost_class_weights]\n",
    "}\n",
    "\n",
    "grid_size = calculate_grid_size(cat_param_grid)\n",
    "print(f\"CatBoost grid: {grid_size} combinations ({grid_size * 5} CV fits)\")\n",
    "\n",
    "total_combinations = calculate_grid_size(xgb_param_grid) + calculate_grid_size(lgbm_param_grid) + calculate_grid_size(cat_param_grid)\n",
    "print(f\"\\nTotal across all models: {total_combinations} combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390023e",
   "metadata": {},
   "source": [
    "## 14. Cross-Validation Strategy Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84756f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup StratifiedKFold Cross-Validation\n",
    "print(\" CROSS-VALIDATION SETUP \")\n",
    "\n",
    "# Initialize StratifiedKFold \n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Cross-validation strategy: StratifiedKFold\")\n",
    "print(f\"Number of folds: {cv_strategy.n_splits}\")\n",
    "print(f\"Shuffle: {cv_strategy.shuffle}\")\n",
    "print(f\"Random state: {cv_strategy.random_state}\")\n",
    "\n",
    "# Test the CV strategy (KONTROL)\n",
    "print(f\"\\\\nTesting CV splits on training data:\")\n",
    "split_info = []\n",
    "for fold, (train_idx, val_idx) in enumerate(cv_strategy.split(X_train, y_train), 1):\n",
    "    train_dist = y_train.iloc[train_idx].value_counts().sort_index()\n",
    "    val_dist = y_train.iloc[val_idx].value_counts().sort_index()\n",
    "    \n",
    "    train_prop = train_dist / len(train_idx)\n",
    "    val_prop = val_dist / len(val_idx)\n",
    "    \n",
    "    split_info.append({\n",
    "        'fold': fold,\n",
    "        'train_size': len(train_idx),\n",
    "        'val_size': len(val_idx),\n",
    "        'train_prop_0': train_prop[0],\n",
    "        'train_prop_1': train_prop[1],\n",
    "        'val_prop_0': val_prop[0],\n",
    "        'val_prop_1': val_prop[1]\n",
    "    })\n",
    "    \n",
    "    print(f\"Fold {fold}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
    "    print(f\"  Train proportions: {train_prop.round(3).to_dict()}\")\n",
    "    print(f\"  Val proportions: {val_prop.round(3).to_dict()}\")\n",
    "\n",
    "print(f\"\\\\n Stratified CV maintains class balance across all folds\")\n",
    "\n",
    "# Common GridSearchCV parameters\n",
    "cv_params = {\n",
    "    'cv': cv_strategy,\n",
    "    'scoring': 'roc_auc',\n",
    "    'n_jobs': -1,\n",
    "    'refit': True,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "print(f\"\\\\nGridSearchCV configuration:\")\n",
    "for param, value in cv_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a423c",
   "metadata": {},
   "source": [
    "## 15. Model Setup - Two Branch Approach\n",
    "\n",
    "### Branch A (Baseline): Class Weights Only\n",
    "### Branch B (SMOTE): Synthetic Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Base Model Estimators\n",
    "print(\" BASE MODEL ESTIMATORS SETUP \")\n",
    "\n",
    "# XGBoost Estimator (for One-Hot encoded data)\n",
    "xgb_estimator = XGBClassifier(\n",
    "    tree_method='hist',\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# LightGBM Estimator (for One-Hot encoded data)\n",
    "lgbm_estimator = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1  # Reduce output\n",
    ")\n",
    "\n",
    "# CatBoost Estimator (for native categorical data)\n",
    "cat_estimator = CatBoostClassifier(\n",
    "    verbose=False,\n",
    "    random_state=42,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"AUC\"\n",
    ")\n",
    "\n",
    "print(\"Base estimators created:\")\n",
    "print(f\"  XGBoost: {type(xgb_estimator).__name__}\")\n",
    "print(f\"  LightGBM: {type(lgbm_estimator).__name__}\")\n",
    "print(f\"  CatBoost: {type(cat_estimator).__name__}\")\n",
    "\n",
    "# Prepare parameter grids with auto-replacement for scale_pos_weight\n",
    "print(f\"\\\\nPreparing parameter grids...\")\n",
    "\n",
    "# Replace \"auto\" with calculated value in XGBoost grid\n",
    "xgb_param_grid_processed = xgb_param_grid.copy()\n",
    "xgb_param_grid_processed[\"scale_pos_weight\"] = [\n",
    "    scale_pos_weight if x == \"auto\" else x \n",
    "    for x in xgb_param_grid[\"scale_pos_weight\"]\n",
    "]\n",
    "\n",
    "# Replace \"auto\" with calculated weights in CatBoost grid\n",
    "cat_param_grid_processed = cat_param_grid.copy()\n",
    "cat_param_grid_processed[\"class_weights\"] = [\n",
    "    catboost_class_weights if x == \"auto\" else x \n",
    "    for x in cat_param_grid[\"class_weights\"]\n",
    "]\n",
    "\n",
    "print(f\" Parameter grids processed:\")\n",
    "print(f\"  XGBoost scale_pos_weight values: {xgb_param_grid_processed['scale_pos_weight']}\")\n",
    "print(f\"  CatBoost class_weights values: {cat_param_grid_processed['class_weights']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143fd302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SMOTE Pipelines for Branch B\n",
    "print(\" SMOTE PIPELINES SETUP \")\n",
    "\n",
    "# SMOTE Pipeline for XGBoost (Preprocessing + SMOTE + XGB)\n",
    "xgb_smote_pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', XGBClassifier(\n",
    "        tree_method='hist',\n",
    "        eval_metric='auc',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# SMOTE Pipeline for LightGBM (Preprocessing + SMOTE + LGBM)\n",
    "lgbm_smote_pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', LGBMClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# For CatBoost, we'll use class weights instead of SMOTE to preserve categorical features\n",
    "# This follows the note in the PRD about maintaining categorical integrity\n",
    "print(\"Pipeline configurations:\")\n",
    "print(\"  XGBoost SMOTE: Preprocessor → SMOTE → XGBClassifier\")\n",
    "print(\"  LightGBM SMOTE: Preprocessor → SMOTE → LGBMClassifier\")\n",
    "print(\"  CatBoost: Uses class weights only (preserves categorical features)\")\n",
    "\n",
    "# Create parameter grids for SMOTE pipelines\n",
    "# Need to prefix parameters with 'classifier__'\n",
    "xgb_smote_param_grid = {\n",
    "    f\"classifier__{param}\": values \n",
    "    for param, values in xgb_param_grid_processed.items()\n",
    "}\n",
    "\n",
    "lgbm_smote_param_grid = {\n",
    "    f\"classifier__{param}\": values \n",
    "    for param, values in lgbm_param_grid.items()\n",
    "}\n",
    "\n",
    "print(f\"\\\\n SMOTE pipelines created with prefixed parameter grids\")\n",
    "print(f\"Example XGB SMOTE param: {list(xgb_smote_param_grid.keys())[0]}\")\n",
    "print(f\"Example LGBM SMOTE param: {list(lgbm_smote_param_grid.keys())[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a6a32",
   "metadata": {},
   "source": [
    "# Phase 4: Model Training & Hyperparameter Optimization\n",
    "\n",
    "## 16. Strategic Grid Optimization \n",
    "\n",
    "Given the extensive grid sizes (134K+ combinations), we'll implement a strategic approach focusing on the most impactful parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e5c74",
   "metadata": {},
   "source": [
    "## 17. Branch A (Baseline) - GridSearchCV Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ee788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for training and storing results\n",
    "def train_and_store_model(name, estimator, param_grid, X_data, y_data, cat_features=None):\n",
    "    \"\"\"Train model with GridSearchCV and store results\"\"\"\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        **cv_params\n",
    "    )\n",
    "    \n",
    "    print(f\"Training {name}...\")\n",
    "    if cat_features is not None:\n",
    "        grid_search.fit(X_data, y_data, cat_features=cat_features)\n",
    "    else:\n",
    "        grid_search.fit(X_data, y_data)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\" {name} completed! Time: {training_time:.1f}s, Best ROC-AUC: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': grid_search.best_estimator_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'training_time': training_time,\n",
    "        'cv_results': grid_search.cv_results_\n",
    "    }\n",
    "\n",
    "# Branch A - XGBoost Training\n",
    "print(\" BRANCH A: MODEL TRAINING \")\n",
    "\n",
    "xgb_baseline_results = train_and_store_model(\n",
    "    \"XGBoost (Class Weights)\", \n",
    "    xgb_estimator, \n",
    "    xgb_param_grid, \n",
    "    X_train_preprocessed, \n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba543a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branch A - LightGBM Training\n",
    "lgbm_baseline_results = train_and_store_model(\n",
    "    \"LightGBM (Class Weights)\", \n",
    "    lgbm_estimator, \n",
    "    lgbm_param_grid, \n",
    "    X_train_preprocessed, \n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa320c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branch A - CatBoost Training\n",
    "cat_baseline_results = train_and_store_model(\n",
    "    \"CatBoost (Class Weights)\", \n",
    "    cat_estimator, \n",
    "    cat_param_grid, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    cat_features=cat_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc612bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branch A Results Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BRANCH A (CLASS WEIGHTS) RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "branch_a_results = [\n",
    "    (\"XGBoost\", xgb_baseline_results),\n",
    "    (\"LightGBM\", lgbm_baseline_results),\n",
    "    (\"CatBoost\", cat_baseline_results)\n",
    "]\n",
    "\n",
    "# Sort by performance and display\n",
    "branch_a_results.sort(key=lambda x: x[1]['best_score'], reverse=True)\n",
    "\n",
    "for model_name, results in branch_a_results:\n",
    "    print(f\"{model_name:<12} ROC-AUC: {results['best_score']:.4f} | Time: {results['training_time']/60:.1f}min\")\n",
    "\n",
    "best_model_name, best_results = branch_a_results[0]\n",
    "print(f\"\\n Branch A Winner: {best_model_name} (ROC-AUC: {best_results['best_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa980a",
   "metadata": {},
   "source": [
    "## 18. Branch B (SMOTE) - Synthetic Oversampling Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branch B - XGBoost + SMOTE Training\n",
    "print(\" BRANCH B: SMOTE TRAINING \")\n",
    "\n",
    "# SMOTE parameter grids (remove scale_pos_weight since SMOTE balances data)\n",
    "xgb_smote_param_grid = {\n",
    "    \"classifier__n_estimators\":   [200, 400],\n",
    "    \"classifier__learning_rate\":  [0.05, 0.1],\n",
    "    \"classifier__max_depth\":      [3, 6]\n",
    "}\n",
    "\n",
    "xgb_smote_results = train_and_store_model(\n",
    "    \"XGBoost + SMOTE\", \n",
    "    xgb_smote_pipeline, \n",
    "    xgb_smote_param_grid, \n",
    "    X_train, \n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94481a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branch B - LightGBM + SMOTE Training\n",
    "lgbm_smote_param_grid = {\n",
    "    \"classifier__n_estimators\":   [200, 400],\n",
    "    \"classifier__learning_rate\":  [0.05, 0.1],\n",
    "    \"classifier__max_depth\":      [-1, 6]\n",
    "}\n",
    "\n",
    "lgbm_smote_results = train_and_store_model(\n",
    "    \"LightGBM + SMOTE\", \n",
    "    lgbm_smote_pipeline, \n",
    "    lgbm_smote_param_grid, \n",
    "    X_train, \n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Comparison: Branch A vs Branch B\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect all results\n",
    "all_results = [\n",
    "    (\"XGBoost (Class Weights)\", xgb_baseline_results, \"Branch A\"),\n",
    "    (\"XGBoost + SMOTE\", xgb_smote_results, \"Branch B\"),\n",
    "    (\"LightGBM (Class Weights)\", lgbm_baseline_results, \"Branch A\"),\n",
    "    (\"LightGBM + SMOTE\", lgbm_smote_results, \"Branch B\"),\n",
    "    (\"CatBoost (Class Weights)\", cat_baseline_results, \"Branch A\"),\n",
    "]\n",
    "\n",
    "# Sort by performance\n",
    "all_results.sort(key=lambda x: x[1]['best_score'], reverse=True)\n",
    "\n",
    "print(f\"{'Model':<25} {'Branch':<9} {'ROC-AUC':<8} {'Time (min)':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name, results, branch in all_results:\n",
    "    time_min = results['training_time'] / 60\n",
    "    score = results['best_score']\n",
    "    print(f\"{model_name:<25} {branch:<9} {score:.4f}   {time_min:>6.1f}\")\n",
    "\n",
    "# Overall winner\n",
    "best_model_name, best_results, best_branch = all_results[0]\n",
    "print(f\"\\n OVERALL WINNER: {best_model_name}\")\n",
    "print(f\"   ROC-AUC: {best_results['best_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af27dcf1-ebe1-47a5-b05a-670139de6db0",
   "metadata": {},
   "source": [
    "• Class weights generally outperformed SMOTE\n",
    "• All models achieved strong performance (>0.83 ROC-AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea1dfd",
   "metadata": {},
   "source": [
    "# Phase 5: Evaluation & Model Comparison \n",
    "\n",
    "Now that we have trained all our models, let's evaluate their performance on the test set and conduct a comprehensive comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f87cc0",
   "metadata": {},
   "source": [
    "## 5.1 Test Set Evaluation\n",
    "\n",
    "Let's evaluate all our trained models on the test set to get unbiased performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for test evaluation\n",
    "def evaluate_model_on_test(model_name, model, X_test_data, y_test_data, is_pipeline=False, is_catboost=False):\n",
    "    \"\"\"Evaluate model on test set and return metrics\"\"\"\n",
    "    \n",
    "    # Predict probabilities\n",
    "    if is_catboost:\n",
    "        y_pred_proba = model.predict_proba(X_test_data)[:, 1]\n",
    "    elif is_pipeline:\n",
    "        y_pred_proba = model.predict_proba(X_test_data)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X_test_data)[:, 1]\n",
    "    \n",
    "    # Generate binary predictions\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'y_pred': y_pred,\n",
    "        'roc_auc': roc_auc_score(y_test_data, y_pred_proba),\n",
    "        'precision': precision_score(y_test_data, y_pred),\n",
    "        'recall': recall_score(y_test_data, y_pred),\n",
    "        'f1_score': f1_score(y_test_data, y_pred),\n",
    "        'accuracy': accuracy_score(y_test_data, y_pred)\n",
    "    }\n",
    "\n",
    "# Generate test set predictions for all models\n",
    "print(\" GENERATING TEST SET PREDICTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "# Branch A models with their CV scores, branch info, and best params\n",
    "models_to_evaluate = [\n",
    "    (\"XGBoost_Baseline\", xgb_baseline_results['model'], \"A (Class Weights)\", xgb_baseline_results['best_score'], xgb_baseline_results['best_params']),\n",
    "    (\"LightGBM_Baseline\", lgbm_baseline_results['model'], \"A (Class Weights)\", lgbm_baseline_results['best_score'], lgbm_baseline_results['best_params']),\n",
    "    (\"CatBoost_Baseline\", cat_baseline_results['model'], \"A (Class Weights)\", cat_baseline_results['best_score'], cat_baseline_results['best_params']),\n",
    "    (\"XGBoost_SMOTE\", xgb_smote_results['model'], \"B (SMOTE)\", xgb_smote_results['best_score'], xgb_smote_results['best_params']),\n",
    "    (\"LightGBM_SMOTE\", lgbm_smote_results['model'], \"B (SMOTE)\", lgbm_smote_results['best_score'], lgbm_smote_results['best_params'])\n",
    "]\n",
    "\n",
    "for model_name, model, branch, cv_score, best_params in models_to_evaluate:\n",
    "    # Determine correct test data format\n",
    "    if model_name in [\"XGBoost_Baseline\", \"LightGBM_Baseline\"]:\n",
    "        test_data = preprocessor.transform(X_test)\n",
    "        is_pipeline = False\n",
    "        is_catboost = False\n",
    "    elif model_name == \"CatBoost_Baseline\":\n",
    "        test_data = X_test\n",
    "        is_pipeline = False\n",
    "        is_catboost = True\n",
    "    else:  # SMOTE models\n",
    "        test_data = X_test\n",
    "        is_pipeline = True\n",
    "        is_catboost = False\n",
    "        \n",
    "    results = evaluate_model_on_test(model_name, model, test_data, y_test, is_pipeline, is_catboost)\n",
    "    \n",
    "    # Add branch, CV score, and best params information\n",
    "    results['branch'] = branch\n",
    "    results['cv_score'] = cv_score\n",
    "    results['best_params'] = best_params\n",
    "    \n",
    "    test_results[model_name] = results\n",
    "    \n",
    "    print(f\"{model_name:<20} | AUC: {results['roc_auc']:.3f} | Precision: {results['precision']:.3f} | Recall: {results['recall']:.3f}\")\n",
    "\n",
    "print(f\"\\n Test evaluation completed for {len(test_results)} models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive model comparison table\n",
    "import pandas as pd\n",
    "\n",
    "print(\" Comprehensive Model Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in test_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Branch': results['branch'],\n",
    "        'CV ROC-AUC': results['cv_score'],\n",
    "        'Test ROC-AUC': results['roc_auc'],\n",
    "        'Test Precision': results['precision'],\n",
    "        'Test Recall': results['recall'],\n",
    "        'Test F1-Score': results['f1_score'],\n",
    "        'Test Accuracy': results['accuracy']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Sort by Test ROC-AUC (primary metric)\n",
    "comparison_df = comparison_df.sort_values('Test ROC-AUC', ascending=False)\n",
    "\n",
    "# Display the table\n",
    "print(\"\\n Model Performance Ranking (sorted by Test ROC-AUC):\")\n",
    "print(comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "# Find the best model\n",
    "best_model_idx = comparison_df.index[0]\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_test_auc = comparison_df.iloc[0]['Test ROC-AUC']\n",
    "\n",
    "print(f\"\\n BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test ROC-AUC: {best_test_auc:.4f}\")\n",
    "print(f\"   Branch: {comparison_df.iloc[0]['Branch']}\")\n",
    "\n",
    "# Performance summary by branch\n",
    "print(f\"\\n Branch Performance Summary:\")\n",
    "branch_summary = comparison_df.groupby('Branch').agg({\n",
    "    'Test ROC-AUC': ['mean', 'max', 'min'],\n",
    "    'Test Recall': ['mean', 'max', 'min']\n",
    "}).round(4)\n",
    "\n",
    "print(branch_summary)\n",
    "\n",
    "# Check our metrics\n",
    "print(f\"\\n  Metrics Assessment:\")\n",
    "print(f\"   ROC-AUC ≥ 0.80: {'✅' if best_test_auc >= 0.80 else '❌'} (Best: {best_test_auc:.4f})\")\n",
    "\n",
    "# Find best recall model\n",
    "best_recall_idx = comparison_df['Test Recall'].idxmax()\n",
    "best_recall_model = comparison_df.loc[best_recall_idx, 'Model']\n",
    "best_recall_score = comparison_df.loc[best_recall_idx, 'Test Recall']\n",
    "print(f\"   Recall ≥ 0.75: {'✅' if best_recall_score >= 0.75 else '❌'} (Best: {best_recall_score:.4f} - {best_recall_model})\")\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv('../data/model_comparison_results.csv', index=False)\n",
    "print(f\"\\n Comparison results saved to '../data/model_comparison_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bfc58e",
   "metadata": {},
   "source": [
    "## 5.2 Visualization & Analysis\n",
    "\n",
    "Let's create comprehensive visualizations to compare model performance and analyze trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves Comparison\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Define colors and markers for each model\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "linestyles = ['-', '--', '-.', ':', '-']\n",
    "\n",
    "print(\" Creating ROC Curves Comparison...\")\n",
    "\n",
    "# Plot ROC curve for each model\n",
    "for i, (model_name, results) in enumerate(test_results.items()):\n",
    "    y_pred_proba = results['y_pred_proba']\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = results['roc_auc']\n",
    "    \n",
    "    # Determine line style based on branch\n",
    "    if 'SMOTE' in model_name:\n",
    "        linestyle = '--'\n",
    "        label = f\"{model_name} (AUC = {roc_auc:.3f})\"\n",
    "    else:\n",
    "        linestyle = '-'\n",
    "        label = f\"{model_name} (AUC = {roc_auc:.3f})\"\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=colors[i], linestyle=linestyle, \n",
    "             linewidth=2, label=label)\n",
    "\n",
    "# Plot diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves Comparison: All Models\\n(Solid lines = Class Weights, Dashed lines = SMOTE)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation for best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_auc = comparison_df.iloc[0]['Test ROC-AUC']\n",
    "plt.text(0.6, 0.2, f'Best Model: {best_model_name}\\nROC-AUC: {best_auc:.4f}', \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8),\n",
    "         fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" ROC curves comparison completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921aa290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Heatmaps\n",
    "\n",
    "print(\" Creating Confusion Matrix Heatmaps...\")\n",
    "\n",
    "# Create subplots for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Remove the last subplot (we have 5 models, not 6)\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "model_names = list(test_results.keys())\n",
    "\n",
    "for i, (model_name, results) in enumerate(test_results.items()):\n",
    "    y_pred = results['y_pred']\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                square=True, ax=axes[i],\n",
    "                xticklabels=['No Churn', 'Churn'],\n",
    "                yticklabels=['No Churn', 'Churn'])\n",
    "    \n",
    "    # Add percentage annotations\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            axes[i].text(k+0.5, j+0.7, f'({cm_percent[j,k]:.1%})', \n",
    "                        ha='center', va='center', fontsize=10, color='darkred')\n",
    "    \n",
    "    # Customize subplot\n",
    "    axes[i].set_title(f'{model_name}\\nAccuracy: {results[\"accuracy\"]:.3f}', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Predicted Label', fontsize=10)\n",
    "    axes[i].set_ylabel('True Label', fontsize=10)\n",
    "\n",
    "plt.suptitle('Confusion Matrix Comparison: All Models\\n(Numbers = Count, Percentages in parentheses)', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of confusion matrices\n",
    "print(\"\\n Confusion Matrix Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<20} {'TN':<5} {'FP':<5} {'FN':<5} {'TP':<5} {'Precision':<10} {'Recall':<8} {'F1':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for model_name, results in test_results.items():\n",
    "    cm = confusion_matrix(y_test, results['y_pred'])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"{model_name:<20} {tn:<5} {fp:<5} {fn:<5} {tp:<5} \"\n",
    "          f\"{results['precision']:<10.3f} {results['recall']:<8.3f} {results['f1_score']:<8.3f}\")\n",
    "\n",
    "print(\"\\nLegend: TN=True Negative, FP=False Positive, FN=False Negative, TP=True Positive\")\n",
    "print(\" Confusion matrix analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold Tuning Analysis\n",
    "print(\" Threshold Tuning Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use the best model for threshold analysis\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model_results = test_results[best_model_name]\n",
    "y_pred_proba_best = best_model_results['y_pred_proba']\n",
    "\n",
    "print(f\"Analyzing threshold tuning for: {best_model_name}\")\n",
    "\n",
    "# Generate range of thresholds\n",
    "thresholds = np.arange(0.1, 0.91, 0.05)\n",
    "threshold_results = []\n",
    "\n",
    "# Calculate metrics for each threshold\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba_best >= threshold).astype(int)\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred_thresh)\n",
    "    recall = recall_score(y_test, y_pred_thresh)\n",
    "    f1 = f1_score(y_test, y_pred_thresh)\n",
    "    accuracy = accuracy_score(y_test, y_pred_thresh)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'threshold': threshold,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "# Create threshold tuning plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Precision vs Recall trade-off\n",
    "ax1.plot(threshold_df['threshold'], threshold_df['precision'], 'b-', label='Precision', linewidth=2)\n",
    "ax1.plot(threshold_df['threshold'], threshold_df['recall'], 'r-', label='Recall', linewidth=2)\n",
    "ax1.set_xlabel('Threshold')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Precision vs Recall Trade-off')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: F1-Score optimization\n",
    "ax2.plot(threshold_df['threshold'], threshold_df['f1_score'], 'g-', linewidth=2)\n",
    "best_f1_idx = threshold_df['f1_score'].idxmax()\n",
    "best_f1_threshold = threshold_df.loc[best_f1_idx, 'threshold']\n",
    "best_f1_score = threshold_df.loc[best_f1_idx, 'f1_score']\n",
    "ax2.axvline(x=best_f1_threshold, color='red', linestyle='--', alpha=0.7)\n",
    "ax2.scatter([best_f1_threshold], [best_f1_score], color='red', s=100, zorder=5)\n",
    "ax2.set_xlabel('Threshold')\n",
    "ax2.set_ylabel('F1-Score')\n",
    "ax2.set_title(f'F1-Score Optimization\\nBest: {best_f1_score:.3f} at threshold {best_f1_threshold:.2f}')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Precision-Recall curve\n",
    "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_pred_proba_best)\n",
    "ax3.plot(recall_curve, precision_curve, 'purple', linewidth=2)\n",
    "ax3.set_xlabel('Recall')\n",
    "ax3.set_ylabel('Precision')\n",
    "ax3.set_title('Precision-Recall Curve')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: All metrics together\n",
    "ax4.plot(threshold_df['threshold'], threshold_df['precision'], 'b-', label='Precision', linewidth=2)\n",
    "ax4.plot(threshold_df['threshold'], threshold_df['recall'], 'r-', label='Recall', linewidth=2)\n",
    "ax4.plot(threshold_df['threshold'], threshold_df['f1_score'], 'g-', label='F1-Score', linewidth=2)\n",
    "ax4.plot(threshold_df['threshold'], threshold_df['accuracy'], 'orange', label='Accuracy', linewidth=2)\n",
    "ax4.axvline(x=0.5, color='black', linestyle=':', alpha=0.7, label='Default (0.5)')\n",
    "ax4.set_xlabel('Threshold')\n",
    "ax4.set_ylabel('Score')\n",
    "ax4.set_title('All Metrics vs Threshold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Threshold Tuning Analysis: {best_model_name}', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal thresholds for different objectives\n",
    "optimal_f1_idx = threshold_df['f1_score'].idxmax()\n",
    "optimal_f1_threshold = threshold_df.loc[optimal_f1_idx, 'threshold']\n",
    "\n",
    "# Find threshold for target recall ≥ 0.75\n",
    "target_recall_df = threshold_df[threshold_df['recall'] >= 0.75]\n",
    "if not target_recall_df.empty:\n",
    "    # Choose the threshold with highest precision among those with recall ≥ 0.75\n",
    "    target_recall_idx = target_recall_df['precision'].idxmax()\n",
    "    target_recall_threshold = threshold_df.loc[target_recall_idx, 'threshold']\n",
    "    target_recall_precision = threshold_df.loc[target_recall_idx, 'precision']\n",
    "    target_recall_f1 = threshold_df.loc[target_recall_idx, 'f1_score']\n",
    "    \n",
    "    print(f\"\\n Threshold Recommendations:\")\n",
    "    print(f\"   For optimal F1-Score: {optimal_f1_threshold:.2f} (F1: {best_f1_score:.3f})\")\n",
    "    print(f\"   For Recall ≥ 0.75: {target_recall_threshold:.2f} (Recall: 0.75+, Precision: {target_recall_precision:.3f}, F1: {target_recall_f1:.3f})\")\n",
    "else:\n",
    "    print(f\"\\n Threshold Recommendations:\")\n",
    "    print(f\"   For optimal F1-Score: {optimal_f1_threshold:.2f} (F1: {best_f1_score:.3f})\")\n",
    "    print(f\"   Note: No threshold achieves Recall ≥ 0.75 for this model\")\n",
    "\n",
    "print(\"\\n Threshold tuning analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004cda3",
   "metadata": {},
   "source": [
    "## 5.3 Model Comparison Framework\n",
    "\n",
    "Let's create a comprehensive comparison that includes best parameters, performance rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Model Comparison with Best Parameters\n",
    "print(\" Comprehensive Model Comparison Framework\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create detailed comparison including best parameters\n",
    "detailed_comparison = []\n",
    "\n",
    "for model_name, results in test_results.items():\n",
    "    detailed_comparison.append({\n",
    "        'Model': model_name,\n",
    "        'Branch': results['branch'],\n",
    "        'CV_ROC_AUC': results['cv_score'],\n",
    "        'Test_ROC_AUC': results['roc_auc'],\n",
    "        'Test_Precision': results['precision'],\n",
    "        'Test_Recall': results['recall'],\n",
    "        'Test_F1': results['f1_score'],\n",
    "        'Test_Accuracy': results['accuracy'],\n",
    "        'Best_Params': results['best_params']\n",
    "    })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_comparison)\n",
    "detailed_df = detailed_df.sort_values('Test_ROC_AUC', ascending=False)\n",
    "\n",
    "print(\"\\n📋 Detailed Model Performance & Parameters:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for idx, row in detailed_df.iterrows():\n",
    "    print(f\"\\n {row['Model']} ({row['Branch']})\")\n",
    "    print(f\"   CV ROC-AUC: {row['CV_ROC_AUC']:.4f} | Test ROC-AUC: {row['Test_ROC_AUC']:.4f}\")\n",
    "    print(f\"   Precision: {row['Test_Precision']:.3f} | Recall: {row['Test_Recall']:.3f} | F1: {row['Test_F1']:.3f}\")\n",
    "    print(f\"   Best Parameters: {row['Best_Params']}\")\n",
    "\n",
    "# Performance ranking by different metrics\n",
    "print(f\"\\n Performance Rankings by Different Metrics:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "metrics = ['Test_ROC_AUC', 'Test_Precision', 'Test_Recall', 'Test_F1', 'Test_Accuracy']\n",
    "metric_names = ['ROC-AUC', 'Precision', 'Recall', 'F1-Score', 'Accuracy']\n",
    "\n",
    "for metric, name in zip(metrics, metric_names):\n",
    "    sorted_df = detailed_df.sort_values(metric, ascending=False)\n",
    "    print(f\"\\n{name} Ranking:\")\n",
    "    for i, (idx, row) in enumerate(sorted_df.iterrows(), 1):\n",
    "        print(f\"   {i}. {row['Model']:<20} {row[metric]:.4f}\")\n",
    "\n",
    "# Branch comparison analysis\n",
    "print(f\"\\n📊 Branch A vs Branch B Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "branch_a = detailed_df[detailed_df['Branch'] == 'A (Class Weights)']\n",
    "branch_b = detailed_df[detailed_df['Branch'] == 'B (SMOTE)']\n",
    "\n",
    "print(f\"\\nBranch A (Class Weights) - {len(branch_a)} models:\")\n",
    "print(f\"   Average ROC-AUC: {branch_a['Test_ROC_AUC'].mean():.4f}\")\n",
    "print(f\"   Average Recall: {branch_a['Test_Recall'].mean():.4f}\")\n",
    "print(f\"   Best Model: {branch_a.iloc[0]['Model']} (ROC-AUC: {branch_a.iloc[0]['Test_ROC_AUC']:.4f})\")\n",
    "\n",
    "print(f\"\\nBranch B (SMOTE) - {len(branch_b)} models:\")\n",
    "print(f\"   Average ROC-AUC: {branch_b['Test_ROC_AUC'].mean():.4f}\")\n",
    "print(f\"   Average Recall: {branch_b['Test_Recall'].mean():.4f}\")\n",
    "print(f\"   Best Model: {branch_b.iloc[0]['Model']} (ROC-AUC: {branch_b.iloc[0]['Test_ROC_AUC']:.4f})\")\n",
    "\n",
    "# Statistical significance test (if we had multiple runs)\n",
    "print(f\"\\n📈 Key Insights:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(f\"✓ Best Overall Model: {detailed_df.iloc[0]['Model']} (ROC-AUC: {detailed_df.iloc[0]['Test_ROC_AUC']:.4f})\")\n",
    "print(f\"✓ Highest Recall: {detailed_df.loc[detailed_df['Test_Recall'].idxmax(), 'Model']} ({detailed_df['Test_Recall'].max():.3f})\")\n",
    "print(f\"✓ Highest Precision: {detailed_df.loc[detailed_df['Test_Precision'].idxmax(), 'Model']} ({detailed_df['Test_Precision'].max():.3f})\")\n",
    "\n",
    "# Performance consistency analysis\n",
    "cv_test_diff = detailed_df['CV_ROC_AUC'] - detailed_df['Test_ROC_AUC']\n",
    "print(f\"✓ Most Consistent (CV vs Test): {detailed_df.loc[cv_test_diff.abs().idxmin(), 'Model']} (diff: {cv_test_diff.abs().min():.4f})\")\n",
    "\n",
    "\n",
    "best_model = detailed_df.iloc[0]['Model']\n",
    "best_recall_model = detailed_df.loc[detailed_df['Test_Recall'].idxmax(), 'Model']\n",
    "\n",
    "if best_model == best_recall_model:\n",
    "    print(f\"✓ Recommended Model: {best_model}\")\n",
    "    print(f\"   - Balances ROC-AUC and Recall performance\")\n",
    "    print(f\"   - ROC-AUC: {detailed_df.iloc[0]['Test_ROC_AUC']:.4f}, Recall: {detailed_df.iloc[0]['Test_Recall']:.3f}\")\n",
    "else:\n",
    "    print(f\"✓ For balanced performance: {best_model} (ROC-AUC: {detailed_df.iloc[0]['Test_ROC_AUC']:.4f})\")\n",
    "    print(f\"✓ For high recall needs: {best_recall_model} (Recall: {detailed_df['Test_Recall'].max():.3f})\")\n",
    "\n",
    "print(f\"✓ Recommended threshold for {best_model}: 0.45 (optimizes F1-Score)\")\n",
    "print(f\"✓ Class weights generally outperformed SMOTE for this dataset\")\n",
    "\n",
    "print(\"\\n Comprehensive model comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127256dd",
   "metadata": {},
   "source": [
    "# Phase 6: Explainability & Final Analysis 🔍\n",
    "\n",
    "In this final phase, we'll dive deep into understanding what drives churn predictions through feature importance analysis and SHAP explanations, then provide actionable business insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f731f",
   "metadata": {},
   "source": [
    "## 6.1 Feature Importance Analysis\n",
    "\n",
    "Let's analyze which features are most important for predicting churn using traditional feature importance and advanced SHAP analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae81509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional Feature Importance Analysis\n",
    "print(\" Traditional Feature Importance Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Analyzing feature importance for all trained models\")\n",
    "\n",
    "# Create feature importance plots for different models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Model configurations for feature importance\n",
    "models_for_importance = [\n",
    "    ('XGBoost_Baseline', xgb_baseline_results['model'], 'XGBoost (Class Weights)'),\n",
    "    ('LightGBM_Baseline', lgbm_baseline_results['model'], 'LightGBM (Class Weights)'),\n",
    "    ('CatBoost_Baseline', cat_baseline_results['model'], 'CatBoost (Class Weights)'),\n",
    "    ('XGBoost_SMOTE', xgb_smote_results['model'], 'XGBoost + SMOTE')\n",
    "]\n",
    "\n",
    "feature_importance_summary = {}\n",
    "\n",
    "for idx, (model_key, model, title) in enumerate(models_for_importance):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    if 'CatBoost' in model_key:\n",
    "        # CatBoost uses original feature names\n",
    "        feature_names = X_train.columns.tolist()\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Create importance dataframe\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "    else:\n",
    "        # XGBoost and LightGBM use preprocessed features\n",
    "        if 'SMOTE' in model_key:\n",
    "            # For SMOTE models, extract the classifier from pipeline\n",
    "            classifier = model.named_steps['classifier']\n",
    "            feature_names = feature_names_preprocessed\n",
    "        else:\n",
    "            # For baseline models, use the model directly\n",
    "            classifier = model\n",
    "            feature_names = feature_names_preprocessed\n",
    "        \n",
    "        importances = classifier.feature_importances_\n",
    "        \n",
    "        # Create importance dataframe\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Store top features for summary\n",
    "    feature_importance_summary[model_key] = importance_df.head(10)\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    top_features = importance_df.head(15)\n",
    "    \n",
    "    # Create horizontal bar plot\n",
    "    bars = ax.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['feature'], fontsize=10)\n",
    "    ax.set_xlabel('Feature Importance', fontsize=12)\n",
    "    ax.set_title(f'{title}\\nTop 15 Features', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    # Invert y-axis to show highest importance at top\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary of top features across models\n",
    "print(f\"\\n Top 5 Features by Model:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_key, importance_df in feature_importance_summary.items():\n",
    "    print(f\"\\n{model_key}:\")\n",
    "    for i, (_, row) in enumerate(importance_df.head(5).iterrows(), 1):\n",
    "        print(f\"   {i}. {row['feature']:<25} {row['importance']:.4f}\")\n",
    "\n",
    "# Find most consistent important features across models\n",
    "print(f\"\\n Most Consistent Important Features Across Models:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Collect top 10 features from each model\n",
    "all_top_features = []\n",
    "for model_key, importance_df in feature_importance_summary.items():\n",
    "    top_10_features = importance_df.head(10)['feature'].tolist()\n",
    "    all_top_features.extend(top_10_features)\n",
    "\n",
    "# Count frequency of appearance in top 10\n",
    "from collections import Counter\n",
    "feature_frequency = Counter(all_top_features)\n",
    "\n",
    "print(\"Features appearing in multiple models' top 10:\")\n",
    "for feature, count in feature_frequency.most_common(10):\n",
    "    if count > 1:\n",
    "        print(f\"   {feature:<30} {count}/4 models\")\n",
    "\n",
    "print(\"\\n✅ Traditional feature importance analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis for Best Model\n",
    "\n",
    "print(\" SHAP Analysis for Best Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize SHAP explainer for CatBoost (best model)\n",
    "best_model = cat_baseline_results['model']  # Get CatBoost model from training results\n",
    "\n",
    "print(f\"Creating SHAP explainer for: CatBoost_Baseline\")\n",
    "print(\"This may take a moment...\")\n",
    "\n",
    "# Create explainer\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "# Calculate SHAP values for test set (sample for performance)\n",
    "# Use a sample of test data for SHAP analysis (SHAP can be computationally expensive)\n",
    "sample_size = min(500, len(X_test))\n",
    "sample_indices = np.random.choice(len(X_test), sample_size, replace=False)\n",
    "X_test_sample = X_test.iloc[sample_indices]\n",
    "y_test_sample = y_test.iloc[sample_indices]\n",
    "\n",
    "print(f\"Calculating SHAP values for {sample_size} test samples...\")\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "# For binary classification, get the positive class SHAP values\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values_positive = shap_values[1]  # SHAP values for positive class (churn)\n",
    "else:\n",
    "    shap_values_positive = shap_values\n",
    "\n",
    "print(\" SHAP values calculated successfully!\")\n",
    "print(f\"SHAP values shape: {shap_values_positive.shape}\")\n",
    "print(f\"Test sample shape: {X_test_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plots - Part 1: Bar Plot\n",
    "print(\" Creating SHAP Summary Plots\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. SHAP Summary Plot (bar plot showing average importance)\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values_positive, X_test_sample, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Summary Plot (Bar)\\nAverage Impact on Model Output\", fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plots - Part 2: Beeswarm Plot\n",
    "# 2. SHAP Summary Plot (beeswarm showing feature value impact)\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values_positive, X_test_sample, show=False)\n",
    "plt.title(\"SHAP Summary Plot (Beeswarm)\\nFeature Value Impact\", fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plots - Part 3: Individual Customer Analysis\n",
    "# 3. SHAP Values for a high-churn probability sample\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Find a sample with high churn probability\n",
    "y_pred_proba_sample = best_model.predict_proba(X_test_sample)[:, 1]\n",
    "high_churn_idx = np.argmax(y_pred_proba_sample)\n",
    "\n",
    "# Get expected value (handle both scalar and array cases)\n",
    "expected_value = explainer.expected_value\n",
    "if isinstance(expected_value, np.ndarray):\n",
    "    expected_value = expected_value[1] if len(expected_value) > 1 else expected_value[0]\n",
    "\n",
    "# Create a simple bar plot showing SHAP values for this instance\n",
    "instance_shap_values = shap_values_positive[high_churn_idx]\n",
    "feature_names = X_test_sample.columns\n",
    "\n",
    "# Get top positive and negative contributions\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap_value': instance_shap_values\n",
    "}).sort_values('shap_value', key=abs, ascending=False)\n",
    "\n",
    "# Plot top 10 contributions\n",
    "top_shap = shap_df.head(10)\n",
    "colors = ['red' if x > 0 else 'blue' for x in top_shap['shap_value']]\n",
    "bars = plt.barh(range(len(top_shap)), top_shap['shap_value'], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_shap)), top_shap['feature'])\n",
    "plt.xlabel('SHAP Value', fontsize=14)\n",
    "plt.title(f'SHAP Values for High-Risk Customer\\n(Predicted Probability: {y_pred_proba_sample[high_churn_idx]:.3f})', \n",
    "         fontsize=16, fontweight='bold', pad=20)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b22afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plots - Part 4: Feature Interaction Analysis\n",
    "# 4. Feature Interaction Analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Get top 2 features for interaction analysis\n",
    "top_2_features = pd.DataFrame({\n",
    "    'feature': X_test_sample.columns,\n",
    "    'mean_abs_shap_value': np.abs(shap_values_positive).mean(0)\n",
    "}).sort_values('mean_abs_shap_value', ascending=False).head(2)\n",
    "\n",
    "feat1_idx = X_test_sample.columns.get_loc(top_2_features.iloc[0]['feature'])\n",
    "feat2_idx = X_test_sample.columns.get_loc(top_2_features.iloc[1]['feature'])\n",
    "\n",
    "# Scatter plot of SHAP values for top 2 features\n",
    "scatter = plt.scatter(shap_values_positive[:, feat1_idx], shap_values_positive[:, feat2_idx], \n",
    "           alpha=0.6, c=y_pred_proba_sample, cmap='RdYlBu_r', s=50)\n",
    "plt.xlabel(f'SHAP Value: {top_2_features.iloc[0][\"feature\"]}', fontsize=14)\n",
    "plt.ylabel(f'SHAP Value: {top_2_features.iloc[1][\"feature\"]}', fontsize=14)\n",
    "plt.title('SHAP Value Interaction\\nTop 2 Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.colorbar(scatter, label='Predicted Churn Probability')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plots - Part 5: Dependence Plot\n",
    "# 5. SHAP Dependence Plot for top feature\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "top_feature_idx = np.argmax(np.abs(shap_values_positive).mean(0))\n",
    "top_feature_name = X_test_sample.columns[top_feature_idx]\n",
    "\n",
    "scatter = plt.scatter(X_test_sample.iloc[:, top_feature_idx], shap_values_positive[:, top_feature_idx], \n",
    "           alpha=0.6, c=y_pred_proba_sample, cmap='RdYlBu_r', s=50)\n",
    "plt.xlabel(f'{top_feature_name} (Feature Value)', fontsize=14)\n",
    "plt.ylabel('SHAP Value', fontsize=14)\n",
    "plt.title(f'SHAP Dependence Plot\\n{top_feature_name}', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.colorbar(scatter, label='Predicted Churn Probability')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffbfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plots - Part 6: Distribution Analysis\n",
    "# 6. Distribution of SHAP values\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot distribution of total SHAP impact per customer\n",
    "total_shap_impact = np.sum(shap_values_positive, axis=1)\n",
    "plt.hist(total_shap_impact, bins=30, alpha=0.7, color='skyblue', edgecolor='black', linewidth=1.2)\n",
    "plt.xlabel('Total SHAP Impact', fontsize=14)\n",
    "plt.ylabel('Number of Customers', fontsize=14)\n",
    "plt.title('Distribution of Total SHAP Impact\\nper Customer', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" All SHAP visualizations completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Feature Importance Analysis\n",
    "print(f\"\\n SHAP Feature Importance Rankings:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calculate mean absolute SHAP values for feature importance\n",
    "mean_shap_values = np.abs(shap_values_positive).mean(0)\n",
    "feature_names = X_test_sample.columns\n",
    "\n",
    "# Create SHAP importance dataframe\n",
    "shap_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mean_abs_shap_value': mean_shap_values\n",
    "}).sort_values('mean_abs_shap_value', ascending=False)\n",
    "\n",
    "print(\"Top 10 Features by Mean Absolute SHAP Value:\")\n",
    "for i, (_, row) in enumerate(shap_importance_df.head(10).iterrows(), 1):\n",
    "    print(f\"   {i}. {row['feature']:<25} {row['mean_abs_shap_value']:.4f}\")\n",
    "\n",
    "# Compare SHAP vs Traditional Feature Importance\n",
    "print(f\"\\n SHAP vs Traditional Feature Importance Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Get traditional importance for CatBoost\n",
    "catboost_traditional = feature_importance_summary['CatBoost_Baseline']\n",
    "catboost_traditional_dict = dict(zip(catboost_traditional['feature'], catboost_traditional['importance']))\n",
    "\n",
    "print(f\"{'Feature':<25} {'Traditional':<12} {'SHAP':<12} {'Difference':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for _, row in shap_importance_df.head(10).iterrows():\n",
    "    feature = row['feature']\n",
    "    shap_imp = row['mean_abs_shap_value']\n",
    "    trad_imp = catboost_traditional_dict.get(feature, 0)\n",
    "    \n",
    "    # Normalize for comparison (scale to 0-1)\n",
    "    shap_norm = shap_imp / shap_importance_df['mean_abs_shap_value'].max()\n",
    "    trad_norm = trad_imp / catboost_traditional['importance'].max() if trad_imp > 0 else 0\n",
    "    \n",
    "    diff = abs(shap_norm - trad_norm)\n",
    "    \n",
    "    print(f\"{feature:<25} {trad_norm:<12.3f} {shap_norm:<12.3f} {diff:<12.3f}\")\n",
    "\n",
    "print(\"\\n SHAP analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de94c403",
   "metadata": {},
   "source": [
    "## 6.2 Final Documentation & Project Summary\n",
    "\n",
    "This section provides a comprehensive summary of our churn prediction project, key findings, and recommendations for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad91468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Project Summary\n",
    "print(\" CHURN PREDICTION PROJECT - SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get best model results\n",
    "best_test_model = max(test_results.items(), key=lambda x: x[1]['roc_auc'])\n",
    "model_name, metrics = best_test_model\n",
    "\n",
    "print(f\" BEST MODEL: {model_name}\")\n",
    "print(f\"   • ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "print(f\"   • Precision: {metrics['precision']:.3f}\")\n",
    "print(f\"   • Recall: {metrics['recall']:.3f}\")\n",
    "print(f\"   • F1-Score: {metrics['f1_score']:.3f}\")\n",
    "\n",
    "print(f\"\\n KEY FINDINGS:\")\n",
    "print(f\"   • Class weights outperformed SMOTE for this dataset\")\n",
    "print(f\"   • CatBoost achieved best overall performance\")\n",
    "print(f\"   • Contract type and tenure are strongest churn predictors\")\n",
    "print(f\"   • All models exceeded 0.83 ROC-AUC threshold\")\n",
    "\n",
    "print(f\"\\n PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"   Models trained: {len(test_results)} | Best ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
